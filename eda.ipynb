{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pyodbc\n",
        "import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sqlalchemy import text\n",
        "from sqlalchemy import create_engine\n",
        "from langdetect import detect, DetectorFactory\n",
        "\n",
        "# Ensure reproducibility for langdetect\n",
        "DetectorFactory.seed = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "server_name = r\"YASSINE\\SQLEXPRESS\"  # Your SQL Server instance\n",
        "mdf_file = r\"D:\\SQLData\\AirbnbDataWarehouse.mdf\"  # Path to your .mdf\n",
        "db_name = \"AirbnbDataWarehouse\"  # Logical database name\n",
        "\n",
        "# --- Connection string for pyodbc ---\n",
        "# Using Trusted Connection (Windows authentication)\n",
        "conn_str = (\n",
        "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
        "    f\"SERVER={server_name};\"\n",
        "    f\"Trusted_Connection=yes;\"\n",
        "    f\"AttachDbFilename={mdf_file};\"\n",
        "    f\"DATABASE={db_name};\"\n",
        ")\n",
        "\n",
        "# --- Attempt connection ---\n",
        "try:\n",
        "    cnxn = pyodbc.connect(conn_str)\n",
        "    print(\"✅ Successfully connected to the database.\")\n",
        "except pyodbc.Error as ex:\n",
        "    print(\"❌ Error connecting to the database:\")\n",
        "    print(ex)\n",
        "    print(\"Check that SQL Server is running, the ODBC driver is installed,\")\n",
        "    print(\"and that the .mdf file is accessible and not already attached.\")\n",
        "    cnxn = None\n",
        "\n",
        "# --- Create SQLAlchemy engine for pandas or other libraries ---\n",
        "if cnxn:\n",
        "    # URL-encode the connection string for SQLAlchemy\n",
        "    params = urllib.parse.quote_plus(conn_str)\n",
        "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
        "    print(\"✅ SQLAlchemy engine created successfully.\")\n",
        "else:\n",
        "    engine = None\n",
        "    print(\"⚠️ SQLAlchemy engine not created due to connection error.\")\n",
        "\n",
        "# --- Optional: Verify connection by fetching top 5 tables (example) ---\n",
        "if engine:\n",
        "    try:\n",
        "        with engine.connect() as connection:\n",
        "            result = connection.execute(text(\"SELECT name FROM sys.tables;\"))\n",
        "            tables = [row[0] for row in result]\n",
        "            print(\"Tables in database:\", tables)  # print full list\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Could not fetch tables:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- List Tables in the Database ---\n",
        "if cnxn:\n",
        "    try:\n",
        "        cursor = cnxn.cursor()\n",
        "        cursor.execute(\"SELECT TABLE_SCHEMA, TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'\")\n",
        "        tables = cursor.fetchall()\n",
        "        print(\"\n",
        "Tables in the database:\")\n",
        "        for table in tables:\n",
        "            print(f\"- {table[0]}.{table[1]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing tables: {e}\")\n",
        "else:\n",
        "    print(\"Cannot list tables: Database connection not established.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load necessary tables into pandas DataFrames\n",
        "try:\n",
        "    df_hosts = pd.read_sql('SELECT * FROM dim_hosts', engine)\n",
        "    df_reviews = pd.read_sql('SELECT * FROM fact_reviews', engine)\n",
        "    df_listings = pd.read_sql('SELECT * FROM dim_listings', engine)\n",
        "    print(\"DataFrames loaded successfully: df_hosts, df_reviews, df_listings\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "\n",
        "print(\"\n",
        "--- df_hosts Info ---\")\n",
        "df_hosts.info()\n",
        "print(\"\n",
        "--- df_reviews Info ---\")\n",
        "df_reviews.info()\n",
        "print(\"\n",
        "--- df_listings Info ---\")\n",
        "df_listings.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Null Value Examination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_null_values(df, df_name):\n",
        "    print(f\"\n",
        "--- Null Values in {df_name} ---\")\n",
        "    null_counts = df.isnull().sum()\n",
        "    null_percentages = (df.isnull().sum() / len(df)) * 100\n",
        "    null_info = pd.DataFrame({'Null Count': null_counts, 'Null Percentage': null_percentages})\n",
        "    null_info = null_info[null_info['Null Count'] > 0].sort_values(by='Null Percentage', ascending=False)\n",
        "    print(null_info)\n",
        "\n",
        "    if not null_info.empty:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "        plt.title(f'Null Values in {df_name}')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No null values found in {df_name}.\")\n",
        "\n",
        "check_null_values(df_hosts, 'df_hosts')\n",
        "check_null_values(df_reviews, 'df_reviews')\n",
        "check_null_values(df_listings, 'df_listings')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. \"Unknown\" Value Examination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_unknown_values(df, df_name, unknown_values=['unknown', 'n/a', '-', 'none', '', 'null', 'not available']):\n",
        "    print(f\"\n",
        "--- 'Unknown' Values in {df_name} ---\")\n",
        "    unknown_counts = {}\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        # Convert to string to handle mixed types gracefully\n",
        "        col_series = df[col].astype(str).str.lower()\n",
        "        count = col_series.isin(unknown_values).sum()\n",
        "        if count > 0:\n",
        "            unknown_counts[col] = count\n",
        "    \n",
        "    if unknown_counts:\n",
        "        unknown_info = pd.Series(unknown_counts).sort_values(ascending=False)\n",
        "        print(unknown_info)\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x=unknown_info.index, y=unknown_info.values, palette='magma')\n",
        "        plt.title(f'Count of \"Unknown\" Values in {df_name}')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xlabel('Column')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No common 'unknown' values found in string columns of {df_name}.\")\n",
        "\n",
        "check_unknown_values(df_hosts, 'df_hosts')\n",
        "check_unknown_values(df_reviews, 'df_reviews')\n",
        "check_unknown_values(df_listings, 'df_listings')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select only numeric columns for correlation analysis\n",
        "numeric_df_listings = df_listings.select_dtypes(include=['number'])\n",
        "\n",
        "if not numeric_df_listings.empty:\n",
        "    correlation_matrix = numeric_df_listings.corr()\n",
        "    print(\"\n",
        "--- Correlation Matrix for df_listings (Numeric Columns) ---\")\n",
        "    print(correlation_matrix)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title('Correlation Matrix of Numeric Columns in df_listings')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No numeric columns found in df_listings for correlation analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Language Detection in Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to safely detect language\n",
        "def safe_detect_language(text):\n",
        "    try:\n",
        "        return detect(str(text))\n",
        "    except:\n",
        "        return 'unknown'\n",
        "\n",
        "# Apply language detection to review comments\n",
        "# Ensure 'comments' column exists and is not entirely null\n",
        "if 'comments' in df_reviews.columns and not df_reviews['comments'].isnull().all():\n",
        "    df_reviews['language'] = df_reviews['comments'].apply(safe_detect_language)\n",
        "    \n",
        "    print(\"\n",
        "--- Distinct Languages in Reviews ---\")\n",
        "    language_counts = df_reviews['language'].value_counts()\n",
        "    print(language_counts)\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    sns.barplot(x=language_counts.index, y=language_counts.values, palette='viridis')\n",
        "    plt.title('Distribution of Languages in Reviews')\n",
        "    plt.xlabel('Language')\n",
        "    plt.ylabel('Number of Reviews')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Percentage of foreign language use in reviews for a particular city (e.g., Paris)\n",
        "    # Assuming 'city' column exists in df_listings and 'listing_id' for joining\n",
        "    # Merge reviews with listings to get city information\n",
        "    if 'listing_id' in df_reviews.columns and 'listing_id' in df_listings.columns and 'city' in df_listings.columns:\n",
        "        df_reviews_with_city = pd.merge(df_reviews, df_listings[['listing_id', 'city']], on='listing_id', how='left')\n",
        "        \n",
        "        target_city = 'Paris' # Example city\n",
        "        target_country_language = 'fr' # French for Paris\n",
        "\n",
        "        paris_reviews = df_reviews_with_city[df_reviews_with_city['city'].str.contains(target_city, case=False, na=False)]\n",
        "\n",
        "        if not paris_reviews.empty:\n",
        "            total_paris_reviews = len(paris_reviews)\n",
        "            foreign_paris_reviews = paris_reviews[paris_reviews['language'] != target_country_language]\n",
        "            num_foreign_paris_reviews = len(foreign_paris_reviews)\n",
        "            \n",
        "            if total_paris_reviews > 0:\n",
        "                percentage_foreign = (num_foreign_paris_reviews / total_paris_reviews) * 100\n",
        "                print(f\"\n",
        "--- Foreign Language Use in {target_city} Reviews ---\")\n",
        "                print(f\"Total reviews in {target_city}: {total_paris_reviews}\")\n",
        "                print(f\"Reviews in languages other than {target_country_language} in {target_city}: {num_foreign_paris_reviews}\")\n",
        "                print(f\"Percentage of foreign language reviews in {target_city}: {percentage_foreign:.2f}%\")\n",
        "            else:\n",
        "                print(f\"No reviews found for {target_city}.\")\n",
        "        else:\n",
        "            print(f\"No listings found for {target_city} to analyze reviews.\")\n",
        "    else:\n",
        "        print(\"Required columns (listing_id, city, comments) not found for language analysis.\")\n",
        "else:\n",
        "    print('comments' column not found or is empty in df_reviews, skipping language detection.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Host Country Correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note the issue: US state abbreviations in 'host_country' column\n",
        "print(\"\n",
        "--- Original 'host_country' values in df_hosts (first 20 unique) ---\")\n",
        "print(df_hosts['host_country'].value_counts().head(20))\n",
        "\n",
        "# Define a mapping for US state abbreviations to 'United States'\n",
        "us_states = {\n",
        "    'AL': 'United States', 'AK': 'United States', 'AZ': 'United States', 'AR': 'United States',\n",
        "    'CA': 'United States', 'CO': 'United States', 'CT': 'United States', 'DE': 'United States',\n",
        "    'FL': 'United States', 'GA': 'United States', 'HI': 'United States', 'ID': 'United States',\n",
        "    'IL': 'United States', 'IN': 'United States', 'IA': 'United States', 'KS': 'United States',\n",
        "    'KY': 'United States', 'LA': 'United States', 'ME': 'United States', 'MD': 'United States',\n",
        "    'MA': 'United States', 'MI': 'United States', 'MN': 'United States', 'MS': 'United States',\n",
        "    'MO': 'United States', 'MT': 'United States', 'NE': 'United States', 'NV': 'United States',\n",
        "    'NH': 'United States', 'NJ': 'United States', 'NM': 'United States', 'NY': 'United States',\n",
        "    'NC': 'United States', 'ND': 'United States', 'OH': 'United States', 'OK': 'United States',\n",
        "    'OR': 'United States', 'PA': 'United States', 'RI': 'United States', 'SC': 'United States',\n",
        "    'SD': 'United States', 'TN': 'United States', 'TX': 'United States', 'UT': 'United States',\n",
        "    'VT': 'United States', 'VA': 'United States', 'WA': 'United States', 'WV': 'United States',\n",
        "    'WI': 'United States', 'WY': 'United States', 'DC': 'United States'\n",
        "}\n",
        "\n",
        "# Create 'corrected_country' column\n",
        "df_hosts['corrected_country'] = df_hosts['host_country'].replace(us_states)\n",
        "\n",
        "# Also handle cases where 'United States' might be written differently\n",
        "df_hosts['corrected_country'] = df_hosts['corrected_country'].replace({'USA': 'United States', 'U.S.A.': 'United States', 'US': 'United States'})\n",
        "\n",
        "print(\"\n",
        "--- 'corrected_country' values in df_hosts (first 20 unique) ---\")\n",
        "print(df_hosts['corrected_country'].value_counts().head(20))\n",
        "\n",
        "# Display changes (example for a few rows)\n",
        "print(\"\n",
        "--- Sample of host_country vs. corrected_country ---\")\n",
        "print(df_hosts[['host_country', 'corrected_country']].drop_duplicates().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Distinct Value Check and Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_and_standardize_column(df, column_name, df_name, threshold=20):\n",
        "    if column_name in df.columns and df[column_name].dtype == 'object':\n",
        "        print(f\"\n",
        "--- Distinct Values for '{column_name}' in {df_name} ---\")\n",
        "        unique_values = df[column_name].value_counts()\n",
        "        print(f\"Total unique values: {len(unique_values)}\")\n",
        "        print(unique_values.head(threshold))\n",
        "\n",
        "        if len(unique_values) > threshold:\n",
        "            print(f\"... (showing top {threshold} out of {len(unique_values)} unique values)\")\n",
        "\n",
        "        # Example of potential standardization (can be expanded)\n",
        "        # For instance, if 'United States' and 'USA' are both present, standardize them.\n",
        "        # This is already partially handled for 'host_country' above.\n",
        "        # For other columns, manual inspection and mapping might be needed.\n",
        "        \n",
        "        # Visualization for top distinct values\n",
        "        if len(unique_values) > 1:\n",
        "            plt.figure(figsize=(12, 7))\n",
        "            sns.barplot(x=unique_values.head(threshold).index, y=unique_values.head(threshold).values, palette='coolwarm')\n",
        "            plt.title(f'Top {threshold} Distinct Values in {column_name} ({df_name})')\n",
        "            plt.xlabel(column_name)\n",
        "            plt.ylabel('Count')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    else:\n",
        "        print(f\"Column '{column_name}' not found or is not of object type in {df_name}.\")\n",
        "\n",
        "# Example columns to check for distinct values and potential standardization\n",
        "check_and_standardize_column(df_hosts, 'host_country', 'df_hosts')\n",
        "check_and_standardize_column(df_hosts, 'host_response_time', 'df_hosts')\n",
        "check_and_standardize_column(df_listings, 'property_type', 'df_listings')\n",
        "check_and_standardize_column(df_listings, 'room_type', 'df_listings')\n",
        "check_and_standardize_column(df_listings, 'bed_type', 'df_listings')\n",
        "check_and_standardize_column(df_listings, 'cancellation_policy', 'df_listings')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Additional Recommended Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\n",
        "--- Additional Analysis Recommendations ---\")\n",
        "print(\"1. **Time-Series Analysis:** Analyze trends in listings, prices, and reviews over time using `fact_calendar` and `dim_dates`.\")\n",
        "print(\"2. **Geospatial Analysis:** Visualize listings on a map to identify popular areas and price distributions (requires latitude/longitude data, likely in `dim_listings`).\")\n",
        "print(\"3. **Price Determinants:** Build a regression model to understand factors influencing listing prices (e.g., `accommodates`, `bedrooms`, `bathrooms`, `property_type`, `amenities`).\")\n",
        "print(\"4. **Host Behavior Analysis:** Examine host response rates, acceptance rates, and superhost status to understand their impact on listing performance.\")\n",
        "print(\"5. **Sentiment Analysis of Reviews:** Beyond language detection, perform sentiment analysis on review comments to gauge guest satisfaction.\")\n",
        "print(\"6. **Availability Analysis:** Investigate listing availability patterns and their correlation with pricing and demand.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "EDA-Inside-Airbnb-data-warehouse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
